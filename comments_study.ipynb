{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\">Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\">Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\">Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка комментариев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пользователи Интернет-магазина могут редактировать и дополнять описания товаров. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Цель: Обучить модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок. Модель должна иметь значение метрики качества *F1* не меньше 0.75. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import notebook\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re \n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение файла с данными и сохранение \n",
    "try: data = pd.read_csv('/datasets/toxic_comments.csv') \n",
    "except: data = pd.read_csv('toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# просмотрим информацию \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yuliaanikeeva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yuliaanikeeva/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/yuliaanikeeva/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# загрузим модули\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "# создадим счётчик, указав в нём стоп-слова\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159561    \"\\nNo he did not, read it again (I would have ...\n",
       "159562    \"\\n Auto guides and the motoring press are not...\n",
       "159563    \"\\nplease identify what part of BLP applies be...\n",
       "159564    Catalan independentism is the social movement ...\n",
       "159565    The numbers in parentheses are the additional ...\n",
       "159566    \":::::And for the second time of asking, when ...\n",
       "159567    You should be ashamed of yourself \\n\\nThat is ...\n",
       "159568    Spitzer \\n\\nUmm, theres no actual article for ...\n",
       "159569    And it looks like it was actually you who put ...\n",
       "159570    \"\\nAnd ... I really don't think you understand...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим текст до обработки\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функции для очистки и лемматизации\n",
    "def clear_text(text):\n",
    "    return \" \".join(re.sub(r'[^a-zA-z]', ' ', text).split())\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join(lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проведем обработку\n",
    "data['text_clear'] = data['text'].apply(clear_text)\n",
    "data['text_lemm'] = data['text_clear'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159561    No he did not read it again I would have thoug...\n",
       "159562    Auto guide and the motoring press are not good...\n",
       "159563    please identify what part of BLP applies becau...\n",
       "159564    Catalan independentism is the social movement ...\n",
       "159565    The number in parenthesis are the additional d...\n",
       "159566    And for the second time of asking when your vi...\n",
       "159567    You should be ashamed of yourself That is a ho...\n",
       "159568    Spitzer Umm there no actual article for prosti...\n",
       "159569    And it look like it wa actually you who put on...\n",
       "159570    And I really don t think you understand I came...\n",
       "Name: text_lemm, dtype: object"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим текст после обработки\n",
    "data['text_lemm'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделим выборку\n",
    "features = data['text_lemm']\n",
    "target = data['toxic']\n",
    "train, valid = train_test_split(data, test_size=0.2)\n",
    "features_train = train['text_lemm']\n",
    "features_valid = valid['text_lemm']\n",
    "target_train = train['toxic']\n",
    "target_valid = valid['toxic']\n",
    "\n",
    "\n",
    "print(features_train.shape, features_valid.shape)\n",
    "print(target_train.shape, target_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (127656, 147426)\n"
     ]
    }
   ],
   "source": [
    "# Вычислим TF-IDF для корпуса текстов\n",
    "corpus = features_train.values\n",
    "tf_idf = count_tf_idf.fit_transform(corpus)\n",
    "print(\"Размер матрицы:\", tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (31915, 147426)\n"
     ]
    }
   ],
   "source": [
    "# вычислим величину TF-IDF для валидационной выборки\n",
    "corpus_valid = features_valid.values\n",
    "tf_idf_valid = count_tf_idf.transform(corpus_valid)\n",
    "print(\"Размер матрицы:\", tf_idf_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7262569832402235"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Логистическая регрессия \n",
    "model = LogisticRegression(random_state=12345)\n",
    "model.fit(tf_idf, target_train)\n",
    "predictions = model.predict(tf_idf_valid)\n",
    "\n",
    "f1 = f1_score(target_valid, predictions)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.719663186196813"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Логистическая регрессия с кросс-вариацией\n",
    "model = LogisticRegression(random_state=12345)\n",
    "scores = cross_val_score(model, tf_idf, target_train, scoring = 'f1', cv=10) \n",
    "final_score = scores.mean()\n",
    "final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.266662\n",
      "[2]\tvalid_0's binary_logloss: 0.248263\n",
      "[3]\tvalid_0's binary_logloss: 0.235654\n",
      "[4]\tvalid_0's binary_logloss: 0.226922\n",
      "[5]\tvalid_0's binary_logloss: 0.220032\n",
      "[6]\tvalid_0's binary_logloss: 0.213619\n",
      "[7]\tvalid_0's binary_logloss: 0.208642\n",
      "[8]\tvalid_0's binary_logloss: 0.20441\n",
      "[9]\tvalid_0's binary_logloss: 0.200447\n",
      "[10]\tvalid_0's binary_logloss: 0.197022\n",
      "[11]\tvalid_0's binary_logloss: 0.19399\n",
      "[12]\tvalid_0's binary_logloss: 0.191296\n",
      "[13]\tvalid_0's binary_logloss: 0.189012\n",
      "[14]\tvalid_0's binary_logloss: 0.18639\n",
      "[15]\tvalid_0's binary_logloss: 0.18437\n",
      "[16]\tvalid_0's binary_logloss: 0.182143\n",
      "[17]\tvalid_0's binary_logloss: 0.180345\n",
      "[18]\tvalid_0's binary_logloss: 0.178533\n",
      "[19]\tvalid_0's binary_logloss: 0.176808\n",
      "[20]\tvalid_0's binary_logloss: 0.175267\n",
      "[21]\tvalid_0's binary_logloss: 0.174127\n",
      "[22]\tvalid_0's binary_logloss: 0.172929\n",
      "[23]\tvalid_0's binary_logloss: 0.171548\n",
      "[24]\tvalid_0's binary_logloss: 0.170141\n",
      "[25]\tvalid_0's binary_logloss: 0.168878\n",
      "[26]\tvalid_0's binary_logloss: 0.167734\n",
      "[27]\tvalid_0's binary_logloss: 0.166487\n",
      "[28]\tvalid_0's binary_logloss: 0.165668\n",
      "[29]\tvalid_0's binary_logloss: 0.164815\n",
      "[30]\tvalid_0's binary_logloss: 0.163808\n",
      "[31]\tvalid_0's binary_logloss: 0.163016\n",
      "[32]\tvalid_0's binary_logloss: 0.162314\n",
      "[33]\tvalid_0's binary_logloss: 0.161422\n",
      "[34]\tvalid_0's binary_logloss: 0.160635\n",
      "[35]\tvalid_0's binary_logloss: 0.159923\n",
      "[36]\tvalid_0's binary_logloss: 0.159272\n",
      "[37]\tvalid_0's binary_logloss: 0.158665\n",
      "[38]\tvalid_0's binary_logloss: 0.157823\n",
      "[39]\tvalid_0's binary_logloss: 0.157213\n",
      "[40]\tvalid_0's binary_logloss: 0.156574\n",
      "[41]\tvalid_0's binary_logloss: 0.155945\n",
      "[42]\tvalid_0's binary_logloss: 0.155265\n",
      "[43]\tvalid_0's binary_logloss: 0.154612\n",
      "[44]\tvalid_0's binary_logloss: 0.154048\n",
      "[45]\tvalid_0's binary_logloss: 0.153224\n",
      "[46]\tvalid_0's binary_logloss: 0.152609\n",
      "[47]\tvalid_0's binary_logloss: 0.151958\n",
      "[48]\tvalid_0's binary_logloss: 0.151435\n",
      "[49]\tvalid_0's binary_logloss: 0.150897\n",
      "[50]\tvalid_0's binary_logloss: 0.150527\n",
      "[51]\tvalid_0's binary_logloss: 0.150102\n",
      "[52]\tvalid_0's binary_logloss: 0.149526\n",
      "[53]\tvalid_0's binary_logloss: 0.14913\n",
      "[54]\tvalid_0's binary_logloss: 0.148869\n",
      "[55]\tvalid_0's binary_logloss: 0.148383\n",
      "[56]\tvalid_0's binary_logloss: 0.147992\n",
      "[57]\tvalid_0's binary_logloss: 0.147803\n",
      "[58]\tvalid_0's binary_logloss: 0.147477\n",
      "[59]\tvalid_0's binary_logloss: 0.147043\n",
      "[60]\tvalid_0's binary_logloss: 0.146648\n",
      "[61]\tvalid_0's binary_logloss: 0.146366\n",
      "[62]\tvalid_0's binary_logloss: 0.145948\n",
      "[63]\tvalid_0's binary_logloss: 0.145439\n",
      "[64]\tvalid_0's binary_logloss: 0.144946\n",
      "[65]\tvalid_0's binary_logloss: 0.14461\n",
      "[66]\tvalid_0's binary_logloss: 0.144331\n",
      "[67]\tvalid_0's binary_logloss: 0.144052\n",
      "[68]\tvalid_0's binary_logloss: 0.143554\n",
      "[69]\tvalid_0's binary_logloss: 0.14317\n",
      "[70]\tvalid_0's binary_logloss: 0.142776\n",
      "[71]\tvalid_0's binary_logloss: 0.142435\n",
      "[72]\tvalid_0's binary_logloss: 0.142065\n",
      "[73]\tvalid_0's binary_logloss: 0.14188\n",
      "[74]\tvalid_0's binary_logloss: 0.14166\n",
      "[75]\tvalid_0's binary_logloss: 0.14125\n",
      "[76]\tvalid_0's binary_logloss: 0.140884\n",
      "[77]\tvalid_0's binary_logloss: 0.140783\n",
      "[78]\tvalid_0's binary_logloss: 0.140532\n",
      "[79]\tvalid_0's binary_logloss: 0.140336\n",
      "[80]\tvalid_0's binary_logloss: 0.140098\n",
      "[81]\tvalid_0's binary_logloss: 0.139806\n",
      "[82]\tvalid_0's binary_logloss: 0.139491\n",
      "[83]\tvalid_0's binary_logloss: 0.139253\n",
      "[84]\tvalid_0's binary_logloss: 0.139039\n",
      "[85]\tvalid_0's binary_logloss: 0.138752\n",
      "[86]\tvalid_0's binary_logloss: 0.138419\n",
      "[87]\tvalid_0's binary_logloss: 0.138186\n",
      "[88]\tvalid_0's binary_logloss: 0.13791\n",
      "[89]\tvalid_0's binary_logloss: 0.137684\n",
      "[90]\tvalid_0's binary_logloss: 0.137585\n",
      "[91]\tvalid_0's binary_logloss: 0.137335\n",
      "[92]\tvalid_0's binary_logloss: 0.137141\n",
      "[93]\tvalid_0's binary_logloss: 0.136929\n",
      "[94]\tvalid_0's binary_logloss: 0.136781\n",
      "[95]\tvalid_0's binary_logloss: 0.136634\n",
      "[96]\tvalid_0's binary_logloss: 0.136522\n",
      "[97]\tvalid_0's binary_logloss: 0.136279\n",
      "[98]\tvalid_0's binary_logloss: 0.136062\n",
      "[99]\tvalid_0's binary_logloss: 0.135918\n",
      "[100]\tvalid_0's binary_logloss: 0.13578\n",
      "[101]\tvalid_0's binary_logloss: 0.135695\n",
      "[102]\tvalid_0's binary_logloss: 0.13552\n",
      "[103]\tvalid_0's binary_logloss: 0.135335\n",
      "[104]\tvalid_0's binary_logloss: 0.135124\n",
      "[105]\tvalid_0's binary_logloss: 0.134964\n",
      "[106]\tvalid_0's binary_logloss: 0.134814\n",
      "[107]\tvalid_0's binary_logloss: 0.134696\n",
      "[108]\tvalid_0's binary_logloss: 0.13456\n",
      "[109]\tvalid_0's binary_logloss: 0.134447\n",
      "[110]\tvalid_0's binary_logloss: 0.134263\n",
      "[111]\tvalid_0's binary_logloss: 0.134098\n",
      "[112]\tvalid_0's binary_logloss: 0.133884\n",
      "[113]\tvalid_0's binary_logloss: 0.133777\n",
      "[114]\tvalid_0's binary_logloss: 0.13356\n",
      "[115]\tvalid_0's binary_logloss: 0.13345\n",
      "[116]\tvalid_0's binary_logloss: 0.133372\n",
      "[117]\tvalid_0's binary_logloss: 0.133254\n",
      "[118]\tvalid_0's binary_logloss: 0.133098\n",
      "[119]\tvalid_0's binary_logloss: 0.133007\n",
      "[120]\tvalid_0's binary_logloss: 0.132809\n",
      "[121]\tvalid_0's binary_logloss: 0.132679\n",
      "[122]\tvalid_0's binary_logloss: 0.132455\n",
      "[123]\tvalid_0's binary_logloss: 0.132375\n",
      "[124]\tvalid_0's binary_logloss: 0.132254\n",
      "[125]\tvalid_0's binary_logloss: 0.132001\n",
      "[126]\tvalid_0's binary_logloss: 0.131872\n",
      "[127]\tvalid_0's binary_logloss: 0.131735\n",
      "[128]\tvalid_0's binary_logloss: 0.131605\n",
      "[129]\tvalid_0's binary_logloss: 0.131516\n",
      "[130]\tvalid_0's binary_logloss: 0.131401\n",
      "[131]\tvalid_0's binary_logloss: 0.13119\n",
      "[132]\tvalid_0's binary_logloss: 0.131173\n",
      "[133]\tvalid_0's binary_logloss: 0.13104\n",
      "[134]\tvalid_0's binary_logloss: 0.130954\n",
      "[135]\tvalid_0's binary_logloss: 0.13084\n",
      "[136]\tvalid_0's binary_logloss: 0.130636\n",
      "[137]\tvalid_0's binary_logloss: 0.130605\n",
      "[138]\tvalid_0's binary_logloss: 0.130421\n",
      "[139]\tvalid_0's binary_logloss: 0.13033\n",
      "[140]\tvalid_0's binary_logloss: 0.130241\n",
      "[141]\tvalid_0's binary_logloss: 0.130191\n",
      "[142]\tvalid_0's binary_logloss: 0.130077\n",
      "[143]\tvalid_0's binary_logloss: 0.130042\n",
      "[144]\tvalid_0's binary_logloss: 0.12998\n",
      "[145]\tvalid_0's binary_logloss: 0.129871\n",
      "[146]\tvalid_0's binary_logloss: 0.129826\n",
      "[147]\tvalid_0's binary_logloss: 0.129653\n",
      "[148]\tvalid_0's binary_logloss: 0.12959\n",
      "[149]\tvalid_0's binary_logloss: 0.12953\n",
      "[150]\tvalid_0's binary_logloss: 0.129418\n",
      "[151]\tvalid_0's binary_logloss: 0.129308\n",
      "[152]\tvalid_0's binary_logloss: 0.129185\n",
      "[153]\tvalid_0's binary_logloss: 0.129137\n",
      "[154]\tvalid_0's binary_logloss: 0.129058\n",
      "[155]\tvalid_0's binary_logloss: 0.128978\n",
      "[156]\tvalid_0's binary_logloss: 0.128925\n",
      "[157]\tvalid_0's binary_logloss: 0.128883\n",
      "[158]\tvalid_0's binary_logloss: 0.128824\n",
      "[159]\tvalid_0's binary_logloss: 0.12872\n",
      "[160]\tvalid_0's binary_logloss: 0.128608\n",
      "[161]\tvalid_0's binary_logloss: 0.12855\n",
      "[162]\tvalid_0's binary_logloss: 0.128453\n",
      "[163]\tvalid_0's binary_logloss: 0.128472\n",
      "[164]\tvalid_0's binary_logloss: 0.128399\n",
      "[165]\tvalid_0's binary_logloss: 0.128306\n",
      "[166]\tvalid_0's binary_logloss: 0.128171\n",
      "[167]\tvalid_0's binary_logloss: 0.128067\n",
      "[168]\tvalid_0's binary_logloss: 0.128012\n",
      "[169]\tvalid_0's binary_logloss: 0.127977\n",
      "[170]\tvalid_0's binary_logloss: 0.127912\n",
      "[171]\tvalid_0's binary_logloss: 0.127878\n",
      "[172]\tvalid_0's binary_logloss: 0.127788\n",
      "[173]\tvalid_0's binary_logloss: 0.127758\n",
      "[174]\tvalid_0's binary_logloss: 0.12775\n",
      "[175]\tvalid_0's binary_logloss: 0.127675\n",
      "[176]\tvalid_0's binary_logloss: 0.127621\n",
      "[177]\tvalid_0's binary_logloss: 0.127525\n",
      "[178]\tvalid_0's binary_logloss: 0.127459\n",
      "[179]\tvalid_0's binary_logloss: 0.127322\n",
      "[180]\tvalid_0's binary_logloss: 0.127238\n",
      "[181]\tvalid_0's binary_logloss: 0.127205\n",
      "[182]\tvalid_0's binary_logloss: 0.127205\n",
      "[183]\tvalid_0's binary_logloss: 0.127098\n",
      "[184]\tvalid_0's binary_logloss: 0.127068\n",
      "[185]\tvalid_0's binary_logloss: 0.127065\n",
      "[186]\tvalid_0's binary_logloss: 0.127034\n",
      "[187]\tvalid_0's binary_logloss: 0.126967\n",
      "[188]\tvalid_0's binary_logloss: 0.126924\n",
      "[189]\tvalid_0's binary_logloss: 0.126918\n",
      "[190]\tvalid_0's binary_logloss: 0.126897\n",
      "[191]\tvalid_0's binary_logloss: 0.126886\n",
      "[192]\tvalid_0's binary_logloss: 0.126834\n",
      "[193]\tvalid_0's binary_logloss: 0.126765\n",
      "[194]\tvalid_0's binary_logloss: 0.126662\n",
      "[195]\tvalid_0's binary_logloss: 0.126633\n",
      "[196]\tvalid_0's binary_logloss: 0.126568\n",
      "[197]\tvalid_0's binary_logloss: 0.126575\n",
      "[198]\tvalid_0's binary_logloss: 0.126535\n",
      "[199]\tvalid_0's binary_logloss: 0.126497\n",
      "[200]\tvalid_0's binary_logloss: 0.126479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7539936102236422"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Градиентный бустинг LightGBM\n",
    "model = lgb.LGBMModel(objective=\"binary\", n_estimators=200,  learning_rate=(0.2), max_depth=10)\n",
    "model.fit(tf_idf, target_train, eval_set=[(tf_idf_valid, target_valid),], eval_metric=\"f1\")\n",
    "\n",
    "predictions = model.predict(tf_idf_valid)\n",
    "pred_bi = [1 if pred >= 0.5 else 0 for pred in predictions]\n",
    "f1 = f1_score(target_valid, pred_bi)\n",
    "f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011550151975683891"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Случайный лес\n",
    "model = RandomForestClassifier(random_state=12345, n_estimators=200, max_depth=30) \n",
    "model.fit(tf_idf, target_train)\n",
    "predictions = model.predict(tf_idf_valid)\n",
    "\n",
    "f1 = f1_score(target_valid, predictions)\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе работы был Обработан текст сообщений: Сначала текст был очищен от знаков препинания и других символов, отличных от букв латинского алфавита, разбит на токены, далее проведена лемматизация с помощью word net, а также создан счётчик и посчитана TF-IDF. \n",
    "\n",
    "Для подготовки предсказаний обучены модели логистической регрессии, случайного леса в классификации и LGBM градиентный бустинг. Для оценки качества использовалась метрика F1. **Наилучший результат F1 (0,75) получен на модели градиентного бустинга LGBM**. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
